# Bayesian_Optimization_Gaussian_Processes

## Project overview:

In this project, I use Bayesian Optimization to independently optimize two material responses of a tire nanocomposite.

The approach follows an iterative process where we initialize and continuously train a Gaussian Process (GP) based on the results of Finite Element Analysis (FEA) simulations of the initial and selected parameters. Initially, five points are randomly generated within the allowable parameter bounds, and the corresponding material responses, y, are obtained from these FEA simulations.

Optimization is then performed in a batch-wise manner. For each batch, 10,000 candidate points are generated by drawing random integers from NumPy. The Expected Improvement (EI) acquisition function is used to evaluate these candidate points, and the best or worst points (depending on maximization or minimization) are selected for FEA simulations to obtain the corresponding responses.

Notably, the batch size in this workflow is set to 5, meaning that the five best perfoming candidates on the acquisition function (EI) points are selected per batch, and optimization proceeds over 10 batches. I run FEA simulations on all best perfoming candidates selected, resulting in a total of 5x10 FEA simulations. This is in addition to the 5 initial simulations required to initialize the GP model.

Throughout the optimization, the results from the current batch of 5 simulations are concatenated with the data from previous batches. We track the best cumulative response over all batches to monitor the optimization progress. To visualize the evolution of the optimization process, I plot the best cumulative response as a function of the batch number (from 1 to 10). Additionally, to better understand how the input space is explored and exploited using the EI function, the selected data points from each batch are projected into the first two principal components and visualized in 2D. This helps illustrate the distribution and progression of the selected input data over the course of the optimization.

Here, I explore independently maximizing the Sound Damping response and minimizing the Material Cost response.

## Description of Functions

To make the code portable, most functionalities are abstracted into functions. The functions used in this code are:

submit_jobs: Submits a batch of jobs to the webserver by filling out a form with parameters (params). It returns a list of URLs where the results can be retrieved from.
job_submission_loop: Loops through an array with the simulation parameters for running a number of simulations subsequently. It calls submit_jobs for each row of the input parameter, and stores the result URLs.
get_results: Polls the result pages for each job until the desired output is found or a timeout is reached. It retrieves the specific optimization output from the results table.
create_candidate_points: Generates random integers as candidate points within specified parameter bounds (param_mins and param_maxs).
expected_improvement: Computes the Expected Improvement (EI) for each candidate point based on the Gaussian Process model, helping select the next set of parameters to optimize. This is the chosen acquisition function, and has the capability of computing the function for maximization and minimization based on the user's defined need.
optimization_step: Chooses the next batch of candidate points to evaluate, based on the expected improvement of the Gaussian Process model, and returns the next points to test. The next points are chosen as the top or bottom 'batchsize' (int) perfoming points on the acquisition function for maximization or minimization, respectively.
plot_evolutions: Plots two plots side by side. On the left the evolution of the best observed value (y_best) over optimization batches. On the right the selected candidate points based on the acquisition function projected onto the space formed by the first two principal components of the data.
GP_initialization: Initializes the Gaussian Process model by submitting initial jobs calling job_submission_loop, retrieving the results calling get_results, and fitting the GP model to the initial training data.
GP_workflow: Implements the full GP-based optimization process, where in each iteration (batch):
new candidate points are explored
new candidates points are evaluated on the acquisition function
a number (set by batch_size) of best performing candidate points are selected based on the acquisition function evaluation
jobs are submitted for the selected best perfoming points from the server
the job results are fetched from the server
the GP is updated on the newly simulated results concatenated to the previous results
the progress of the optimization (i.e., the progress of the best response and the best performing parameters on a reduced space are visualized for each batch)

	
## Job Specific Inputs

n_batches: number of best performing candidates selected based on the evaluation of the acquisition function
n_candidate_pts: number of candidates to generate randomly to evaluate the acquisition function on
batchsize: number of times we go through the optimization loop
param_mins: lower bounds for the inputs
param_maxs: upper bounds for the inputs
output_list: list of material responses the server outputs
X_train: 5 input points to initialize the optimization on


## Notes on the Optimized Process:

The cumulative best (minimum) response does not decrease throughout the optimization, indicating that the Gaussian Process (GP) does not explore better performing region in the parameter space. This is an indication that exploration within the current batch scheme using the expected impovement acquisition function is not broad enough. As in the previous optimization process, we do not show local fluctuations in the performance of the optimized response (y_best). So for this process, we surprisingly obtain the minimum material cost during training for one of those five initialization training points. The X_train data, which is updated throughout the optimization, seems to be somewhat broadly distributed, at least when viewed through the first two principal components (PCs). This distribution might suggest that the process does not find good regions to exploit, and keeps on moving along the parameter space without luck. Some options to potentially overcome this issue would be changing the batch-wise scheme to choose a single candidate per move, thus having 50 optimization steps, rather than choosing 5 candidates reducing the optimization to 10 steps (the latter is the approach followed here).

The best 'material_cost' resulting from the simulations ran for the training set and its response (and microstructure) can be found at:

http://abaqus.oit.duke.edu:8000/status/jp630/777

The parameters for which this result was found are:

[v_time, v_temp, x_s, x_cb, mix, grade] = [301, 422, 4, 7, 1, 3]
